---
title: 'Assignment #3 - PALMA'
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries}
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(themis)            # for step functions for unbalanced data
library(doParallel)        # for parallel processing
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(patchwork)         # for combining plots nicely
library(ranger)
library(xgboost)
theme_set(theme_minimal()) # Lisa's favorite theme
```

```{r data}
data("lending_club")
# Data dictionary (as close as I could find): https://www.kaggle.com/wordsforthewise/lending-club/discussion/170691
```

## Put it on GitHub!        

Website Link: **https://github.com/apalma127/assignment-3**

## Modeling

The outcome we are interested in predicting is `Class`. And according to the dataset's help page, its values are "either 'good' (meaning that the loan was fully paid back or currently on-time) or 'bad' (charged off, defaulted, or 21-120 days late)".

**Tasks:** 

*1.* Explore the data, concentrating on examining distributions of variables and examining missing values. 

**Quantitative Variables**

```{r}
lending_club %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(variable), 
             scales = "free")
```

Points of Interest:

- all_util and revol_util are pretty normally shaped which is good
- acc_now_delinq looks like all values are clustered at or around 0 ... remove variable maybe? Same w delinq amount.... or are they MISSING???
- int_rate tapers a little ... skew right but nothing crazy
- literally every single other quant variable skews right and should get some adjusting
- inq_last_6mths appears to be more categorical than quant...


**Categorical Variables**

```{r}
lending_club %>%
select(where(is.factor)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_bar() +
  facet_wrap(vars(variable), 
             scales = "free", 
             nrow = 2)
```

Points of Interest:

- for CLASS -- IMPORTANT -- appears to be wayyyyy too much good vs bad.... need to address in recipe or prior
- too many sub grade categories ... should aim to make more concise
- emp length is pretty evenly distributed .... 
- term appears to be way more skewed to 36 than 60

```{r}
lending_club %>%
  count(sub_grade)
```


*2.* Split the data into training and test, putting 75\% in the training data. Stratify by `Class` (add `strata = `Class` to the `initial_split()` function).

```{r}
set.seed(494) # for reproducibility

lending_split <- initial_split(lending_club, 
                             prop = .75, strata = "Class")

lending_training <- training(lending_split)
lending_test <- testing(lending_split)
```


*3.* Set up the recipe and the pre-processing steps to build a lasso model. Some steps you should take:

Once you have that, use `prep()`, `juice()`, and `count()` to count the number of observations in each class. They should be equal. This dataset will be used in building the model, but the data without up and down sampling will be used in evaluation.

```{r}
set.seed(456)

lasso_recipe <- recipe(Class ~ ., 
                       data = lending_training) %>% 
  step_upsample(Class, over_ratio = 0.5) %>%
  step_downsample(Class, under_ratio = 1) %>%
  step_mutate_at(all_numeric(), 
                fn = ~ as.numeric(.)) %>%
  step_mutate(sub_grade = as.character(sub_grade),
                sub_grade =
                        case_when(
                          sub_grade %in% paste("A",1:6, sep = "") ~ "A",
                          sub_grade %in% paste("B",1:6, sep = "") ~ "B",
                          sub_grade %in% paste("C",1:6, sep = "") ~ "C",
                          sub_grade %in% paste("D",1:6, sep = "") ~ "D",
                          sub_grade %in% paste("E",1:6, sep = "") ~ "E",
                          sub_grade %in% paste("F",1:6, sep = "") ~ "F",
                          sub_grade %in% paste("G",1:6, sep = "") ~ "G",
                          TRUE ~ sub_grade), 
              sub_grade = as.factor(sub_grade)) %>%
  step_dummy(all_nominal(), 
             -all_outcomes()) %>% 
  step_normalize(all_predictors(), 
                 -all_nominal())
```

```{r}
lasso_recipe %>% 
  prep(lending_training) %>%
  juice() 
```


*4.* Set up the lasso model and workflow. We will tune the `penalty` parameter.

```{r}
lasso_mod  <- 
  logistic_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("classification")

lasso_wf <-  workflow() %>% 
  add_recipe(lasso_recipe) %>% 
  add_model(lasso_mod)
```


*5.* Set up the model tuning for the `penalty` parameter. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack. Find the accuracy and area under the roc curve for the model with the best tuning parameter.  Use 5-fold cv.

Tune:

```{r}
set.seed(494) #for reproducible 5-fold
lending_cv <- vfold_cv(lending_training, v = 5)


penalty_grid <- grid_regular(penalty(),
                             levels = 10)

ctrl_grid <- control_stack_grid()

lasso_tune <-  
  lasso_wf %>% 
  tune_grid(
    resamples = lending_cv,
    grid = penalty_grid,
    control = ctrl_grid)

lasso_tune %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  filter(.metric == "accuracy")

```


```{r}

lasso_tune %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  scale_x_log10(
   breaks = scales::trans_breaks("log10", function(x) 10^x),
   labels = scales::trans_format("log10",scales::math_format(10^.x))) +
  labs(x = "penalty", y = "accuracy")
```

```{r}
lasso_tune %>% 
  show_best(metric = "accuracy")
```

```{r}
best_param <- lasso_tune %>% 
  select_best(metric = "accuracy")
best_param
```

```{r}
lasso_tune %>%
  select_best(metric = "accuracy")

lasso_tune %>%
  show_best(metric = "accuracy") %>%
  filter(.config == "Preprocessor1_Model09")

lasso_tune %>%
  show_best(metric = "roc_auc") %>%
  filter(.config == "Preprocessor1_Model09")
```

**ACCURACY = 0.7183403**

**ROC AUC = 0.7348244 **


*6.* Set up the recipe and the pre-processing steps to build a random forest model. You shouldn't have to do as many steps. The only steps you should need to do are making all integers numeric and the up and down sampling. 

```{r}
set.seed(456)

rf_recipe <-  
  recipe(Class ~ ., 
        data = lending_training) %>% 
  step_upsample(Class, over_ratio = 0.5) %>%
  step_downsample(Class, under_ratio = 1) %>%
  step_mutate_at(all_numeric(), 
                fn = ~ as.numeric(.))
  
```


*7.* Set up the random forest model and workflow. We will tune the `mtry` and `min_n` parameters and set the number of trees, `trees`, to 100 (otherwise the next steps take too long).

```{r}
rf_model <- rand_forest(mtry = tune(), 
              min_n = tune(), 
              trees = 100) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

rf_workflow <- workflow() %>% 
  add_recipe(rf_recipe) %>% 
  add_model(rf_model) 
```

*8.* Set up the model tuning for both the `mtry` and `min_n` parameters. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack. Use only 3 levels in the grid. For the `mtry` parameter, you need to put `finalize(mtry(), lending_training %>% select(-Class))` in as an argument instead of just `mtry()`, where `lending_training` is the name of your training data. This is because the `mtry()` grid will otherwise have unknowns in it. This part can take a while to run.

```{r}
rf_penalty_grid <- grid_regular(
  finalize(mtry(), lending_training %>% select(-Class)),
  min_n(),
  levels = 3)


rf_tune <- 
  rf_workflow %>% 
  tune_grid(
    resamples = lending_cv, 
    grid = rf_penalty_grid, 
    control = control_stack_grid())

```

*9.* Find the best tuning parameters. What are the accuracy and area under the ROC curve for the model with those tuning parameters?

```{r}
rf_tune %>%
  select_best(metric = "accuracy")

rf_tune %>%
  collect_metrics(metric = "accuracy") %>%
  filter(.config == "Preprocessor1_Model2")

```


**Accuracy = 0.9284379**

**ROC AUC = 0.7183883**


*10.* Next, we will fit a boosted tree using xgboost. We will only tune the `learn_rate` parameter. I have specified the model, recipe, and workflow below already (uncomment the code - you can this by highlighting it and then in the code tab at the top, choose comment/uncomment lines). You need to set up a grid of ten values for the tuning parameter and tune the model. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack.

```{r}
xgboost_spec <-
  boost_tree(
    trees = 1000,
    min_n = 5,
    tree_depth = 2,
    learn_rate = tune(),
    loss_reduction = 10^-5,
    sample_size = 1) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgboost_recipe <- recipe(formula = Class ~ ., data = lending_training) %>%
  step_upsample(Class, over_ratio = .5) %>%
  step_downsample(Class, under_ratio = 1) %>%
  step_mutate_at(all_numeric(),
                 fn = ~as.numeric(.)) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_zv(all_predictors())

xgboost_workflow <-
  workflow() %>%
  add_recipe(xgboost_recipe) %>%
  add_model(xgboost_spec)

set.seed(494)
registerDoParallel() 

boost_penalty_grid <- grid_regular(
  learn_rate(),
  levels = 10)

boost_tune <- xgboost_workflow %>% 
    tune_grid(
    resamples = lending_cv, 
    grid = boost_penalty_grid, 
    control = control_stack_grid())
```

*11.* Find the best tuning parameters. What are the accuracy and area under the ROC curve for the model with those tuning parameters?

```{r}
boost_tune %>%
  select_best(metric = "accuracy")

boost_tune %>%
  collect_metrics(metric = "accuracy") %>%
  filter(.config == "Preprocessor1_Model10")

```

**ACCURACY = 0.8037070**

**ROC AUC = 0.6816321**

*12.* Create a model stack with the candidate models from the previous parts of the exercise and use the `blend_predictions()` function to find the coefficients of the stacked model. Create a plot examining the performance metrics for the different penalty parameters to assure you have captured the best one. If not, adjust the penalty. (HINT: use the `autoplot()` function). 

```{r}
lending_stack <- stacks() %>% 
  add_candidates(lasso_tune) %>% 
  add_candidates(rf_tune) %>% 
  add_candidates(boost_tune)
```

```{r}
lending_blend <- lending_stack %>%
            blend_predictions()

lending_blend
```


**The greatest contributors to the stacking are the log_reg lasso tune (1_09, by far the most compared to the rest) followed by random forest tune (1_4, whose impact is half of that of the log reg lasso).  Overall, the log_reg contributes the most by far...**


```{r}
lending_blend$metrics %>% 
  filter(.metric == "accuracy")
```

```{r}
autoplot(lending_blend)

```

**No adjustment necessary as the num of members is minimized all while maximizing accuracy and roc auc**


13. Fit the final stacked model using `fit_members()`. Apply the model to the training data. 

Compute the accuracy, 

construct a confusion matrix, 

```{r}
lending_final_stack <- lending_blend %>%
                    fit_members(data = lending_training)
```

```{r}
lending_final_stack$metrics %>% 
  filter(.metric == "accuracy")
```

**Accuracy = 0.9488284**

```{r}
lending_final_stack %>%
  conf_mat(obs, pred)

```


```{r}
ggplot(lending_final_stack, aes(x = .pred_good, fill = Class)) +
  geom_density() 
```

and create a density plot with `.pred_good` on the x-axis (the probability of a response of "good"), filled by `Class`. Comment on what you see. 



*14.* In the previous problem, you saw that although the accuracy was quite high, the true negative rate (aka sensitivity) was terrible. It's common to see this when one of the classes has low representation. What we want to do now is investigate what happens in each of our models. Below I've provided code to investigate the lasso model (where `lasso_tune` is the name of my tuning step). Do similar things for the random forest and xgboost models. 

```{r}
lasso_tune %>% 
  collect_predictions() %>% 
  group_by(id, penalty) %>% 
  summarize(accuracy = sum((Class == .pred_class))/n(),
            true_neg_rate = sum(Class == "bad" & .pred_class == "bad")/sum(Class == "bad"),
            true_pos_rate = sum(Class == "good" & .pred_class == "good")/sum(Class == "good")) %>% 
  group_by(penalty) %>% 
  summarize(across(accuracy:true_pos_rate, mean))
```

```{r}
rf_tune %>% 
  collect_predictions() %>% 
  group_by(id, mtry, min_n) %>% 
  summarize(accuracy = sum((Class == .pred_class))/n(),
            true_neg_rate = sum(Class == "bad" & .pred_class == "bad")/sum(Class == "bad"),
            true_pos_rate = sum(Class == "good" & .pred_class == "good")/sum(Class == "good")) %>% 
  group_by(mtry, min_n) %>% 
  summarize(across(accuracy:true_pos_rate, mean))
```

```{r}
boost_tune %>% 
  collect_predictions() %>% 
  group_by(id, learn_rate) %>% 
  summarize(accuracy = sum((Class == .pred_class))/n(),
            true_neg_rate = sum(Class == "bad" & .pred_class == "bad")/sum(Class == "bad"),
            true_pos_rate = sum(Class == "good" & .pred_class == "good")/sum(Class == "good")) %>% 
  group_by(learn_rate) %>% 
  summarize(across(accuracy:true_pos_rate, mean))
```



I would choose:

**Lasso -- penalty == 7.742637e-02, accuracy = 0.7183403, w PEAK accuracy, neg_rate, and pos rate**

**RF -- mtry == 1, min_n == 40, accuracy = 0.8223750, neg_rate is maximized and pos rate and quite high still**

**Boost -- learn_rate == 1e-02, accuracy = 0.70562681 and neg_rate and pos_rate are maximized w a moderately strong accuracy**


**One way to automate: when selecting metrics and tuning parameters, I would filter for true_neg_rate being higher than what I want to improve upon... with also a minimum basement val for accuracy to ensure you are not forfeiting a lot of accuracy too.**


## Shiny app - DONE

Shiny links on our course [Resource](https://advanced-ds-in-r.netlify.app/resources.html)

**Tasks:**

1. How can you save a model you built to use it later (like in the shiny app you'll create)?

**Just like from shiny using anim_save(), with a model you can use saveRDS (I think?) or just save -->  save(model, file="model.Rdata") ... which can be brought back with the load function**

2. For shiny apps that get published (like yours will), it's very important to have ALL the libraries that are used within the app loaded. If we were going to use the stacked model, which libraries do you think we'd need to load in our app?  

**I would imagine we need tidyverse, tidymodels, DEFINITELY stacks, depending on the model --glmnet, ranger, kknn; naniar, lubridate, vip**


3. You'll want the user to be able to choose values for each variable in the model. How will you come up with the values they can choose for quantitative and categorical data? Give one example for each, either using code or in words.  


**I can use graphical processes like the preprocessing work we have done for quantitative and categorical variables. By doing so, I can eye ball right off the bat some ranges of inputs for things, especially binary outputs.  I can use the count function for categorical variables to see the breakdown and for quantitative I can sort the data and/or simply find the min and max values from the category to establish a user input range.**


4. You will need to populate each variable with an initial value. Which value will you choose? Is there a nice way to do this programatically (ie. with code)?


**For each variable, I think it would make sense to start with the neutral case and go from there to avoid any appearance of guiding the user.  This would mean all categories that are extra and that come after receiving a loan would be set to 0 ie delinq variables, total_bal, open_il...others like funded amt, interest rate, and annual income will be randomly generated in a reasonable range allowing for the other variables to play off that.  Programmatically, we can set some to 0 at the start easily and others can simply be randomized using random generator**


## Coded Bias - DONE

We will be watching some of the [Coded Bias](https://www.codedbias.com/) film together on Thursday. It is streaming on Netflix. Write a short reflection.
 

**As a poli sci major, I was most intrigued by the Hong Kong pro-democracy protests and how they eliminated the effects of facial recognition in the streets.  First off, I was unaware of how prominent the security forces in Hong Kong were displaying facial recognition tech on the streets.  I can't believe they were flaunting it.  I also can't believe they were using it to apprehend those peacefully protesting for freedom and their own sovereignty.  I am even more surprised and impressed by the ingenuity of this brave people to do whatever it took to deter and block the facial recognition from aiding in their kidnapping.  It truly is amazing to see such marginalized people (by the PRC) step up to the plate and defeat an algorithm that was being used for bad.**
